import math
import torch
from torch import nn, Tensor
from torch.nn import Parameter
from torch.nn.init import xavier_normal_, constant_


class HyperLinear(nn.Module):
    """
    The Hyper Network module
    """

    def __init__(self, primary_model_hidden_dim, primary_model_dynamic_dim, hyper_net_dim, num_classes):
        super().__init__()
        self.weight_in_dim = primary_model_hidden_dim
        self.weight_out_dim = num_classes

        self.primary_hidden_dim = primary_model_dynamic_dim
        self.hyper_hidden_dim = hyper_net_dim

        # primary weight(weight_in_dim, weight_out_dim) generated by hyper network
        self.weight_w1 = Parameter(torch.fmod(torch.randn(self.primary_hidden_dim, self.weight_in_dim*self.hyper_hidden_dim),2))
        self.weight_b1 = Parameter(torch.fmod(torch.randn(self.weight_in_dim*self.hyper_hidden_dim),2))

        self.weight_w2 = Parameter(torch.fmod(torch.randn(self.hyper_hidden_dim, self.weight_out_dim),2))
        self.weight_b2 = Parameter(torch.fmod(torch.randn(self.weight_out_dim),2))

        # primary bias(weight_out_dim) generated by hyper network
        self.bias_w1 = Parameter(torch.fmod(torch.randn(self.primary_hidden_dim, self.hyper_hidden_dim), 2))
        self.bias_b1 = Parameter(torch.fmod(torch.randn(self.hyper_hidden_dim), 2))

        self.bias_w2 = Parameter(torch.fmod(torch.randn(self.hyper_hidden_dim, self.weight_out_dim), 2))
        self.bias_b2 = Parameter(torch.fmod(torch.randn(self.weight_out_dim), 2))

    def forward(self, data):
        """
        the Hyper Network forward call
        """
        weight_in = torch.matmul(data, self.weight_w1) + self.weight_b1
        batch_size = data.shape[0]
        weight_in = weight_in.view(batch_size, self.weight_in_dim, self.hyper_hidden_dim)

        weight_final = torch.matmul(weight_in, self.weight_w2) + self.weight_b2
        hyper_weight = weight_final.view(batch_size, self.weight_in_dim, self.weight_out_dim)

        bias_in = torch.matmul(data, self.bias_w1) + self.bias_b1
        bias_in = bias_in.view(batch_size, self.hyper_hidden_dim)

        bias_final = torch.matmul(bias_in, self.weight_w2) + self.weight_b2
        hyper_bias = bias_final.view(batch_size, 1, self.weight_out_dim)

        primary_weight = hyper_weight + hyper_bias

        return primary_weight


class Classifier(nn.Module):
    def __init__(self, n_hid, n_out):
        super(Classifier, self).__init__()
        self.n_hid    = n_hid
        self.n_out    = n_out
        self.linear   = nn.Linear(n_hid,  n_out, bias=False)
    def forward(self, x):
        tx = self.linear(x)
        return torch.sigmoid(tx.squeeze())
    def __repr__(self):
        return '{}(n_hid={}, n_out={})'.format(
            self.__class__.__name__, self.n_hid, self.n_out)


class DUET(nn.Module):
    def __init__(self, args):
        super(DUET, self).__init__()
        self.cuda_condition = torch.cuda.is_available() and not args.no_cuda
        self.device = torch.device("cuda:{}".format(args.gpu_id) if self.cuda_condition else "cpu")
        self.item_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)
        self.position_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=args.hidden_size,
                                                        nhead=args.num_attention_heads,
                                                        dim_feedforward=4 * args.hidden_size,
                                                        dropout=args.attention_probs_dropout_prob,
                                                        activation=args.hidden_act)
        self.item_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=args.num_hidden_layers)
        self.LayerNorm = nn.LayerNorm(args.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(args.hidden_dropout_prob)

        self.classifier = Classifier(args.hidden_size, args.item_size)
        self.criterion = nn.BCELoss(reduction='none')

        self.gru_layers = nn.GRU(
            input_size=args.hidden_size,
            hidden_size=args.hidden_size,
            num_layers=2,
            bias=False,
            batch_first=True,
        )
        self.hyper_net_list = nn.ModuleList()
        self.num_generator = args.num_generator
        for i in range(self.num_generator):
            self.hyper_net_list.append(HyperLinear(args.hidden_size, args.hidden_size, args.hidden_size, args.item_size))

        self.betas = (args.adam_beta1, args.adam_beta2)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=args.lr, betas=self.betas, weight_decay=args.weight_decay)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=args.lr_dc_step, gamma=args.lr_dc)
        self.args = args
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """ Initialize the weights """
        stdv = 1.0 / math.sqrt(self.args.hidden_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def generate_square_subsequent_mask(self, sz: int) -> Tensor:
        r"""Generate a square mask for the sequence. The masked positions are filled with float('-inf').            Unmasked positions are filled with float(0.0).        """

        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, -10000.0).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, batch):
        input_ids = batch[1]

        key_padding_mask = (input_ids == 0)
        attn_mask = self.generate_square_subsequent_mask(self.args.max_seq_length).to(input_ids.device)

        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        item_embeddings = self.item_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        sequence_emb = item_embeddings + position_embeddings
        sequence_emb = self.LayerNorm(sequence_emb)
        sequence_emb = self.dropout(sequence_emb)
        seq_hidden_permute = sequence_emb.permute(1, 0, 2)

        encoded_layers = self.item_encoder(seq_hidden_permute,
                                           mask=attn_mask,
                                           src_key_padding_mask=key_padding_mask)

        sequence_output = encoded_layers.permute(1, 0, 2).contiguous()
        return sequence_output

    def pretrain_stage(self, batch):
        seq_output = self.forward(batch)
        return seq_output

    def hyper_stage(self, batch, tau=1):
        seq_output = self.forward(batch)
        seq_emb = seq_output
        dynamic_emb = seq_output[:, -self.args.dynamic_length:,:].detach()
        hyper_emb, _ = self.gru_layers(dynamic_emb)
        hyper_emb = hyper_emb[:, -1, :]
        primary_weight_list = []

        for i in range(self.num_generator):
            weight = self.hyper_net_list[i](hyper_emb).unsqueeze(1)
            primary_weight_list.append(weight)
        bsz = hyper_emb.shape[0]
        primary_weight = torch.cat(primary_weight_list, 1)
        primary_weight = primary_weight.view(bsz, self.num_generator, -1)
        primary_weight_t = primary_weight.permute(0, 2, 1)
        weight_sim = torch.bmm(primary_weight, primary_weight_t).sum(-1)/tau

        weight_sim_row = weight_sim.sum(-1, keepdim=True)
        weight_sim = weight_sim/weight_sim_row
        weight_sim_soft = torch.softmax(weight_sim, -1).view(bsz, self.num_generator, -1)
        weight_kernel = weight_sim_soft * primary_weight
        weight_kernel = weight_kernel.sum(1).view(bsz, self.args.hidden_size, self.args.item_size)

        class_score = torch.bmm(seq_emb, weight_kernel).squeeze()
        logits = torch.sigmoid(class_score.squeeze())
        return logits

    def base_stage(self, batch):
        seq_output = self.forward(batch)
        return seq_output

    def base_pretrain_ft_stage(self, batch):
        seq_output = self.forward(batch)
        return seq_output

